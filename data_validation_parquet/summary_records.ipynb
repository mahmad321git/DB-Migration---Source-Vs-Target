{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType\n",
    "from pyspark.sql import Row, Column\n",
    "import datetime\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import calendar\n",
    "import uuid\n",
    "import time\n",
    "from dateutil import relativedelta\n",
    "from datetime import timedelta\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "BATCH=True\n",
    "BUCKET='oracle-ligands-stats-data'\n",
    "####################################################################\n",
    "\n",
    "SUMMARY_RECORDS_COLUMNS = StructType([\n",
    "    StructField(\"table_name\", StringType(), True),\n",
    "    StructField(\"lower_bound\", IntegerType(), True),\n",
    "    StructField(\"upper_bound\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"total_count\", StringType(), True),\n",
    "    StructField(\"failed_count\", StringType(), True)\n",
    "   ])\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "if BATCH:\n",
    "    spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='s3://'+BUCKET+'/summary_records/*/*'\n",
    "print(path)\n",
    "df_output=spark.read.format(\"com.databricks.spark.csv\").option(\"header\", 'false').schema(SUMMARY_RECORDS_COLUMNS).option(\"delimiter\", ',').load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.sort('table_name', 'lower_bound').show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opath='s3://'+BUCKET+'/final_report/'\n",
    "df_output.repartition(1).write.option(\"delimiter\", ',').option(\"header\", \"false\").option(\"quoteAll\", \"true\").option(\"quote\", \"\\\"\").csv(opath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
