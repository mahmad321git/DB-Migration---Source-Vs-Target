{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType\n",
    "from pyspark.sql import Row, Column\n",
    "import datetime\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import calendar\n",
    "import uuid\n",
    "import time\n",
    "from dateutil import relativedelta\n",
    "from datetime import timedelta\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframe(df1, df2):\n",
    "    '''\n",
    "    function: compare_dataframe\n",
    "    description: Compares two dataframes and show their differences.\n",
    "\n",
    "    Args:\n",
    "        df1: spark.sql.dataframe - First dataframe to be compared.\n",
    "        df2: spark.sql.dataframe - Second dataframe to be compared.\n",
    "\n",
    "    returns:\n",
    "        df1subdf2: spark.sql.dataframe - Different rows of df1.\n",
    "        df2subdf1: spark.sql.dataframe - Different rows of df2.\n",
    "    '''\n",
    "    df1subdf2 = df1.subtract(df2)\n",
    "    df2subdf1 = df2.subtract(df1)\n",
    "\n",
    "    print('Different rows in first dataframe.')\n",
    "    df1subdf2.show()\n",
    "\n",
    "    print('Different rows in second dataframe.')\n",
    "    df2subdf1.show()\n",
    "\n",
    "    return df1subdf2, df2subdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch')\n",
    "parser.add_argument('--bucket')\n",
    "parser.add_argument('--lower_bound')\n",
    "parser.add_argument('--upper_bound')\n",
    "parser.add_argument('--table_name')\n",
    "parser.add_argument('--oracle_data_path')\n",
    "parser.add_argument('--postgres_data_path')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "#BATCH = args.batch\n",
    "#BUCKET = args.bucket\n",
    "#LOWER_BOUND = args.lower_bound\n",
    "#UPPER_BOUND = args.upper_bound\n",
    "#TABLE_NAME = args.table_name\n",
    "#ORACLE_DATA_PATH = 's3://'+BUCKET+'/'+args.oracle_data_path\n",
    "#POSTGRES_DATA_PATH = 's3://'+BUCKET+'/'+args.postgres_data_pathh\n",
    "\n",
    "####################################################################\n",
    "BATCH=True\n",
    "BUCKET='oracle-ligands-stats-data'\n",
    "LOWER_BOUND = 129\n",
    "UPPER_BOUND = 150\n",
    "TABLE_NAME='dnatag.ligands_stats'\n",
    "ORACLE_DATA_PATH = 's3://'+BUCKET+'/oracle/129-150/DNATAG/LIGANDS_STATS'\n",
    "POSTGRES_DATA_PATH = 's3://'+BUCKET+'/postgres/129-150/dnatag/ligands_stats'\n",
    "#POSTGRES_DATA_PATH = 's3://oracle-ligands-stats-data/postgres-1/123/dnatag_wip/ligands_stats_123'\n",
    "####################################################################\n",
    "\n",
    "UPDATED=datetime.datetime.today().replace(second=0, microsecond=0)\n",
    "SUMMARY_RECORDS = 's3://'+BUCKET+'/summary_records/'+TABLE_NAME+'_'+str(LOWER_BOUND)+'_'+str(UPPER_BOUND)\n",
    "INVALID_RECORDS = 's3://'+BUCKET+'/invalid_records/'+TABLE_NAME+'_'+str(LOWER_BOUND)+'_'+str(UPPER_BOUND)\n",
    "SUMMARY_RECORDS_COLUMNS = ['table_name', 'lower_bound', 'upper_bound', 'status', 'timestamp', 'total_count', 'failed_count']\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "if BATCH:\n",
    "    spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ORACLE_DATA_PATH)\n",
    "print(POSTGRES_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oracle=spark.read.parquet(ORACLE_DATA_PATH)\n",
    "df_postgres=spark.read.parquet(POSTGRES_DATA_PATH)\n",
    "\n",
    "#df_oracle.show(5)\n",
    "#df_postgres.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_oracle, df_compare_postgres = compare_dataframe(df_oracle,df_postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORACLE_COMPARE_COUNT=df_compare_oracle.count()\n",
    "POSTGRES_COMPARE_COUNT=df_compare_postgres.count()\n",
    "print(ORACLE_COMPARE_COUNT)\n",
    "print(POSTGRES_COMPARE_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORACLE_COUNT=df_oracle.count()\n",
    "print(ORACLE_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (ORACLE_COMPARE_COUNT == 0 & POSTGRES_COMPARE_COUNT == 0 ):\n",
    "    status=\"Success\"\n",
    "    SUMMARY_RECORDS_VALS = [ (\"dnatag.ligands_stats\", LOWER_BOUND, UPPER_BOUND, status,UPDATED,ORACLE_COUNT, 0)]\n",
    "else:\n",
    "    status=\"Failure\"\n",
    "    count=max(ORACLE_COMPARE_COUNT, POSTGRES_COMPARE_COUNT)\n",
    "    SUMMARY_RECORDS_VALS = [ (\"dnatag.ligands_stats\", LOWER_BOUND, UPPER_BOUND, status,UPDATED, ORACLE_COUNT, count)]\n",
    "    df_compare_oracle.repartition(1).write.option(\"delimiter\", ',').option(\"header\", \"false\").option(\"quoteAll\", \"true\").option(\"quote\", \"\\\"\").csv(INVALID_RECORDS+'/oracle')\n",
    "    df_compare_postgres.repartition(1).write.option(\"delimiter\", ',').option(\"header\", \"false\").option(\"quoteAll\", \"true\").option(\"quote\", \"\\\"\").csv(INVALID_RECORDS+'/postgres')\n",
    "\n",
    "df_output = spark.createDataFrame(SUMMARY_RECORDS_VALS, SUMMARY_RECORDS_COLUMNS)\n",
    "df_output.show()\n",
    "\n",
    "df_output.repartition(1).write.option(\"delimiter\", ',').option(\"header\", \"false\").option(\"quoteAll\", \"true\").option(\"quote\", \"\\\"\").csv(SUMMARY_RECORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
